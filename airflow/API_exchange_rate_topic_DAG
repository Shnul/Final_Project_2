from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
import requests
import json
from kafka import KafkaProducer

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'station1_API_exchange_rate_topic',
    default_args=default_args,
    description='A DAG to fetch exchange rates and send to Kafka',
    schedule=timedelta(days=1),  # Updated to use `schedule` instead of `schedule_interval`
)

def run_exchange_rate_script():
    # Initialize Spark session with MinIO checkpoint configuration
    spark = SparkSession.builder \
        .appName("Exchange Rate API to Kafka") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio-server:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "I6wT68mGbJ0Q1DqC") \
        .config("spark.hadoop.fs.s3a.secret.key", "OOKyQYPYIm9PegMFM8mLd8wq2fT7sB7K") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.sql.streaming.checkpointLocation", "s3a://checkpoints/API_exchange_rate_topic/API_exchange_rate_topic/") \
        .getOrCreate()
    print("Spark session initialized successfully")

    # Function to make an HTTP GET request
    def get_exchange_rate():
        url = "https://v6.exchangerate-api.com/v6/2fbf16c377a5875d9c8785c5/latest/USD"
        response = requests.get(url)
        print("HTTP GET request successful")
        return response.json()

    # Function to send data to Kafka
    def send_to_kafka(topic, data):
        producer = KafkaProducer(bootstrap_servers='course-kafka:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))
        producer.send(topic, data)
        producer.flush()
        print("Data sent to Kafka successfully")

    # Main function
    # Get exchange rate data
    exchange_rate_data = get_exchange_rate()
    print("Exchange rate data:", exchange_rate_data)
    
    # Send the exchange rate data to Kafka
    send_to_kafka('exchange_rate_topic', exchange_rate_data)
    print("Data sent to Kafka successfully")

    # Stop the Spark session
    spark.stop()
    print("Spark session stopped successfully")

run_exchange_rate_task = PythonOperator(
    task_id='run_exchange_rate_script',
    python_callable=run_exchange_rate_script,
    dag=dag,
)

run_exchange_rate_task
