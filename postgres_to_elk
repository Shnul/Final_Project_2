from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Paths to the required JAR files
jdbc_driver_path = "/opt/spark/jars/postgresql-42.2.20.jar"
elasticsearch_connector_path = "/opt/spark/jars/elasticsearch-spark-30_2.12-8.5.0.jar"

# Create Spark session
spark = SparkSession.builder \
    .appName("PostgresToElasticsearch") \
    .config("spark.master", "local") \
    .config("spark.es.nodes", "final_project-elasticsearch-1") \
    .config("spark.es.port", "9200") \
    .config("spark.jars", f"{jdbc_driver_path},{elasticsearch_connector_path}") \
    .getOrCreate()

# JDBC URL and properties
jdbc_url = "jdbc:postgresql://final_project-postgres-1:5432/airflow"
jdbc_properties = {
    "user": "postgres",
    "password": "postgres",
    "driver": "org.postgresql.Driver"
}

# Read data from PostgreSQL
df = spark.read \
    .jdbc(jdbc_url, "exchange_rates", properties=jdbc_properties)

# Cast decimal columns to string
df = df.select([col(c).cast("string") if t == "decimal" else col(c) for c, t in df.dtypes])

# Write data to Elasticsearch
df.write \
    .format("org.elasticsearch.spark.sql") \
    .option("es.resource", "exchange_rates/_doc") \
    .save()

# Stop Spark session
spark.stop()
